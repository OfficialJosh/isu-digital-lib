Title,Abstract,Academic or Administrative Unit,Type,Copyright,Keywords,DOI,Permanent Link,Collections,Date,Authors,Advisors,File,Committee Member,Subject Categories,Publisher
Fairness specification and repair for machine learning pipeline,"Machine learning (ML) algorithms are increasingly used in critical decision-making systems, such as criminal sentencing, employee hiring, bank loan approvals, and college admissions, all of which have a direct impact on human lives. The fairness of these ML-based systems has become a significant concern in recent years. Numerous incidents have highlighted instances where ML models have discriminated against individuals based on protected attributes like race, gender, age, and religious beliefs. While considerable research has been devoted to identifying and mitigating unfairness in ML models, these methods lack generalizability as they only perform effectively in specific datasets, ML algorithms, or fairness metrics. This dissertation introduces novel techniques for optimizing fairness within machine learning pipelines, addressing the limitations of existing bias mitigation approaches. In the first approach, we introduce Fair-AutoML, the first generalized method for addressing fairness bugs (optimizing fairness) using AutoML. Unlike existing bias mitigation techniques, Fair-AutoML addresses their limitations by enabling efficient and fairness-aware Bayesian search to repair unfair models, making it effective for a wide range of datasets, models, and fairness metrics. There are two innovative contributions of Fair-AutoML, including a dynamic optimization function and a fairness-aware search space. By dynamically adjusting the optimization function to balance accuracy and fairness, we can effectively reduce bias with little to no impact on accuracy. Moreover, our fairness-aware search space pruning method allows Fair-AutoML to enhance fairness with minimal computational cost and repair time. In machine learning tasks, it is standard practice to construct a pipeline comprising an ordered set of stages, including data acquisition, preprocessing, modeling, and more. However, designing ML pipelines that adhere to fairness standards is challenging due to their inherent complexity. Various components, such as data preprocessing and feature engineering, play a crucial role in determining the system’s overall fairness. While numerous bias mitigation and testing techniques have been developed to enhance fairness in ML pipelines, they often fail to detect and explain the root causes of bias effectively. We propose two novel methods, Fairness Contract and Fairness Checker, which apply design-by-contract (DbC) principles to ensure algorithmic fairness within the ML pipeline, enabling the detection and explanation of bias within the ML pipeline. First, we introduce a preventive mechanism called ”Fairness Contract,” which can detect fairness violations in real-time as the ML program executes. Its modular design enables us to identify the precise location of fairness violations within the ML software. Second, we introduce a novel approach, Fairness Checker, which uses concentration inequalities to identify bias in the ML pipeline. Our method starts by calculating fairness scores for the dataset features, revealing which features are positively or negatively affected after the pipeline’s preprocessing steps. These findings are then applied to concentration inequalities to uncover bias issues within the ML pipeline. Furthermore, we created a fairness annotator that enables developers to specify and enforce modular fairness throughout the pipeline’s development process.",Department of Computer Science,dissertation,,Automated machine learning,,https://dr.lib.iastate.edu/handle/20.500.12876/jw27n5Gv,Theses and Dissertations,2024-12,"Nguyen, Giang","Rajan, Hridesh",Nguyen_iastate_0097E_21787.pdf,,Computer science,
,,,,,Bias mitigation,,,,,,"Le, Wei",,,,
,,,,,Design-by-contract,,,,,,"Cai, Ying",,,,
,,,,,Fairness,,,,,,"Prabhu, Gurpur",,,,
,,,,,Machine Learning,,,,,,"Li, Qi",,,,
,,,,,Probabilistic specifications,,,,,,"Mitra, Simanta",,,,